							-*- mode: outline -*-

* GEM/Stub

* Stages

** Geometry

** Tesselation

** Compute

* Fixed function

** Depth buffers

HiZ.

** Stencil

** Blending

* Thread pool

Group primitives per tile, maybe fixed size per-tile queue, then run all shaders for
one tile on one cpu-core.  Should reduce contention, allow one cpu to have entire
RT tile (page) in cache the whole time.

* JIT

** Cache programs if JITer gets too slow

** JIT in ps thread setup code

We can generate code to copy in the exact payload we'll need. Also
compile in depth test. Maybe even entire tile level loop to avoid
function pointer dispatch per SIMD8 group. This should use the
register allocator so that ideally for small shader we might never
need to write out the payload.  Compile in RT write, blending, srgb
conversion.

** Track constants per sub reg (use case: msg headers)

** Detect constant offset ubo loads

Since we JIT at 3DPRIMITIVE time, we know which buffers are bound when
we JIT, so if we see a constant offset load through sampler or constant
cache (both read only cached) we can just look up the constant and compile
it into the shader as a regular load.

* WM

** SIMD16 dispatch

** Use AVX2

** Perspective correct barycentric

** Multi-sample

** Lines, points

* EU

** Use immediate AVX2 shift when EU operand is an immediate

** All the instructions

** Indirect addressing

** Control flow

** Write masks

Can ignore outside control flow, inside control flow we can use avx2
blend to implement write masking.

** Execution size greater than 8

* Misc

** Hybrid HW passthrough mode.

Run on hw for a while, then switch to simulator when something
triggers. Switch back.

* KIR

** Fix tail call optimization for eot sends/calls. For example, this:

00a8  send(8)         g124<1>UW       g13<8,8,1>F
                            sampler ld_lz SIMD8 Surface = 1 Sampler = 0 mlen 3 rlen 4 { align1 1Q };
00b8  sendc(8)        null<1>UW       g124<8,8,1>F
                            render RT write SIMD8 LastRT Surface = 0 mlen 4 rlen 0 { align1 1Q EOT };

becomes

       const_send src g13-g15, dst g124-g127  lea    -0x10e7(%rip),%rsi        # 0x0000000000000000
                                            push   %rdi
                                            callq  0x00000000082a44a2
                                            pop    %rdi
       send src g124-g127                   lea    -0x10b5(%rip),%rsi        # 0x0000000000000040
                                            push   %rdi
                                            callq  0x00000000082a0a76
                                            pop    %rdi
       eot                                  retq   

which could just be

       const_send src g13-g15, dst g124-g127  lea    -0x10e7(%rip),%rsi        # 0x0000000000000000
                                            callq  0x00000000082a44a2
       send src g124-g127                   lea    -0x10b5(%rip),%rsi        # 0x0000000000000040
                                            jmp  0x00000000082a0a76

since we don't need rdi anymore at that point and we can instead of
call for the last call that ends the thread.

** Detect same vb strides

Only compute each vid * stride once, so that buffers with the same
stride don't generate the same computation.

** Spill registers that hold regions or immediates

When choosing a register to spill, try to find one that holds an
immediate, and avoid spilling. Make unspill just reload the value. Can
be done for regions too, but needs analysis to determine that region
is unchanged. Maybe rewrite grf access to be SSA?


** Better region xfer copy prop

When we compile in code to copy constants into the shader regs we get
code like:

# --- code emit
# load attribute deltas
r0   = load_region g236.0<8,8,1>4           vmovdqa 0x1d80(%rdi),%ymm0
       store_region r0, g2.0<8,8,1>4        vmovdqa %ymm0,0x40(%rdi)
r1   = load_region g237.0<8,8,1>4           vmovdqa 0x1da0(%rdi),%ymm1
       store_region r1, g3.0<8,8,1>4        vmovdqa %ymm1,0x60(%rdi)
# eu ps
r2   = load_region g2.12<0,1,0>4            vpbroadcastd 0x4c(%rdi),%ymm2
       store_region r2, g124.0<8,8,1>4      vmovdqa %ymm2,0xf80(%rdi)
r3   = load_region g2.28<0,1,0>4            vpbroadcastd 0x5c(%rdi),%ymm3
       store_region r3, g125.0<8,8,1>4      vmovdqa %ymm3,0xfa0(%rdi)
r4   = load_region g3.12<0,1,0>4            vpbroadcastd 0x6c(%rdi),%ymm4
       store_region r4, g126.0<8,8,1>4      vmovdqa %ymm4,0xfc0(%rdi)
r5   = load_region g3.28<0,1,0>4            vpbroadcastd 0x7c(%rdi),%ymm5
       store_region r5, g127.0<8,8,1>4      vmovdqa %ymm5,0xfe0(%rdi)
       send src g124-g127                   lea    -0x144b(%rip),%rsi        # 0x0000000000000060
                                            push   %rdi
                                            callq  0xffffffffff741749
                                            pop    %rdi
       eot                                  retq   

This inlines and unrolls the copy, but sine we load uniforms from the
constants, the regions don't match and we don't propagate into the
later loads.  It would be even better if we could rewrite the loads
from, eg g2.12<0,1,0>4 to load directly from g236.12<0,1,0>4 and avoid
the intermediate stores.

** Pre-populate registers with common loads

For example, we have a bit of masking and shift to generate the frag
coord hearder in grf1, and then an awkward region load to load in a
different order. We can pre-load a register with the fragcord region
in a more efficient way (construct from x and y) and alias the
fragcoord region to it.  Also just for uniform loads from attribute
deltas.

** Create fragcoord header in PS

This allows us to optimize it out for shaders (most) that don't use
fragcoord.

** Maintain pointer to current pixel for rt0

Or maybe just an offset from the surface base. Updating the offset
incrementally is a lot less code than computing a y tile offset from
scratch for every pixel.
